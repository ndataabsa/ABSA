{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25ae352e-37ae-43b2-9f83-7d424a726236",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import re\n",
    "import os\n",
    "\n",
    "from atoss.process import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c95d4dd7-54ee-466c-b796-e681d7173dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_O(group):\n",
    "    return (group == 'O').sum()\n",
    "def count_Os(group):\n",
    "    return (group == 'OOOO').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5915e95-8f6d-4ef1-97cf-94bd261ce881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asqp_rest15 : 4263 data add. Total number: 4263\n",
      "asqp_rest16 : 4491 data add. Total number: 8754\n",
      "acos_rest16 : 4374 data add. Total number: 13128\n",
      "acos_laptop16 : 2163 data add. Total number: 15291\n",
      "finished and save data ==>> /home/elicer/ABSA/outputs/mvp/f1_train_zero_quad.xlsx\n",
      "asqp_rest15 : 4263 data add. Total number: 4263\n",
      "asqp_rest16 : 4491 data add. Total number: 8754\n",
      "acos_rest16 : 4374 data add. Total number: 13128\n",
      "acos_laptop16 : 2163 data add. Total number: 15291\n",
      "finished and save data ==>> /home/elicer/ABSA/outputs/mvp/f1_train_zero_item.xlsx\n",
      "asqp_rest15 : 736 data add. Total number: 736\n",
      "asqp_rest16 : 668 data add. Total number: 1404\n",
      "acos_rest16 : 812 data add. Total number: 2216\n",
      "acos_laptop16 : 619 data add. Total number: 2835\n",
      "finished and save data ==>> /home/elicer/ABSA/outputs/mvp/f1_dev_zero_quad.xlsx\n",
      "asqp_rest15 : 736 data add. Total number: 736\n",
      "asqp_rest16 : 668 data add. Total number: 1404\n",
      "acos_rest16 : 812 data add. Total number: 2216\n",
      "acos_laptop16 : 619 data add. Total number: 2835\n",
      "finished and save data ==>> /home/elicer/ABSA/outputs/mvp/f1_dev_zero_item.xlsx\n"
     ]
    }
   ],
   "source": [
    "# compute f1 score by task, dataset, type(quad/item) \n",
    "method = 'mvp'\n",
    "llm = 'zero'\n",
    "\n",
    "data_types = ['train', 'dev']\n",
    "f1_types = ['quad', 'item']\n",
    "task_datasets = [['asqp','rest15'],['asqp','rest16'],['acos','rest16'],['acos','laptop16']]\n",
    "data_counts =  [[834, 606, 723, 2934],[209, 277, 133, 326]]\n",
    "home = os.path.expanduser('~/')\n",
    "\n",
    "\n",
    "for data_type in data_types:\n",
    "    for f1_type in f1_types:\n",
    "        fi_score = pd.DataFrame()\n",
    "        for task_dataset in task_datasets:\n",
    "            task, dataset = task_dataset[0], task_dataset[1]\n",
    "\n",
    "            if method == 'mvp':\n",
    "                output_dir = os.path.join(home, f'ABSA/outputs/{method}/{task}/{dataset}/top_5_post_data1.0')\n",
    "            elif method == 'dlo':\n",
    "                output_dir = os.path.join(home, f'ABSA/outputs/{method}/{task}/{dataset}/top_5_post_data1.0')\n",
    "            elif method == 'paraphrase':\n",
    "                output_dir = os.path.join(home, f'ABSA/outputs/{method}/{task}/{dataset}/post_data1.0')\n",
    "            \n",
    "            for i in range(0, 11):\n",
    "        \n",
    "                file_name = f'{task}_{dataset}_{data_type}_{llm}_{i}.xlsx'\n",
    "                file_path = os.path.join(output_dir, file_name)\n",
    "                df = pd.read_excel(file_path)\n",
    "                if f1_type == 'quad':\n",
    "                    df = df.groupby(['sent_id', 'sentence']).agg({'max_ord_t': 'max','max_ord_p': 'max',\n",
    "                                                                  'score': count_Os}).reset_index()\n",
    "                    df['score'] = df['score'].fillna(0)\n",
    "                    df['precision'] = df['score']/(df['max_ord_p'])\n",
    "                    df['recall'] = df['score']/(df['max_ord_t'])\n",
    "                elif f1_type == 'item':\n",
    "                    df = df.groupby(['sent_id', 'sentence']).agg({'max_ord_t': 'max','max_ord_p': 'max',\n",
    "                                                                  'A_err': count_O,'C_err': count_O,'O_err':count_O,'S_err':count_O}).reset_index()   \n",
    "                    df['score'] = df['A_err'] + df['C_err'] + df['O_err']+ df['S_err']\n",
    "                    df['score'] = df['score'].fillna(0)\n",
    "                    df['precision'] = df['score']/(df['max_ord_p']*4)\n",
    "                    df['recall'] = df['score']/(df['max_ord_t']*4)\n",
    "                df['f1'] = 2 * df['precision'] * df['recall'] / (df['precision'] + df['recall'])\n",
    "                df['f1'] = df['f1'].fillna(0)\n",
    "                df['f1'] = round(df['f1'] * 100, 2)\n",
    "                df.rename(columns={'f1': f'f1_{i}'}, inplace=True)\n",
    "                df.rename(columns={'sentence': f'sent_{i}'}, inplace=True)\n",
    "                if i == 0:\n",
    "                    f1_df = df[['sent_id','max_ord_t',f'sent_{i}',f'f1_{i}']]\n",
    "                else:\n",
    "                    f1_df[f'sent_{i}'] = df[f'sent_{i}']\n",
    "                    f1_df[f'f1_{i}'] = df[f'f1_{i}']\n",
    "        \n",
    "            f1_df['model'] = f'{task}_{dataset}'\n",
    "            \n",
    "            f1_df['dataset'] = ''\n",
    "            start_index = 0\n",
    "            \n",
    "            if data_type == 'train':\n",
    "                data_count = data_counts[0]\n",
    "            elif data_type == 'dev':\n",
    "                data_count = data_counts[1]\n",
    "            \n",
    "            for count, name in zip(data_count, task_datasets):\n",
    "                task1, dataset1 = name[0], name[1]\n",
    "                if task!=task1 or dataset!=dataset1:\n",
    "                    end_index = start_index + count\n",
    "                    f1_df.loc[start_index:end_index-1, 'dataset'] = f'{task1}_{dataset1}'\n",
    "                    f1_df.loc[start_index:end_index-1, 'data_id'] = range(1, count + 1)\n",
    "                    start_index = end_index\n",
    "        \n",
    "            col_names = list(f1_df.columns)\n",
    "            f1_col = sorted([col for col in col_names if col.startswith('f1_')], key=lambda x: int(x.split('_')[1]))\n",
    "            sent_col = sorted([col for col in col_names if col.startswith('sent_') and col != 'sent_id'], key=lambda x: int(x.split('_')[1]))\n",
    "            sorted_col = ['model','dataset','data_id','sent_id', 'max_ord_t'] + f1_col + sent_col\n",
    "            \n",
    "            f1_df = f1_df[sorted_col]\n",
    "            fi_score = pd.concat([fi_score, f1_df], ignore_index=True)\n",
    "            print(f'{task}_{dataset} : {len(f1_df)} data add. Total number: {len(fi_score)}')\n",
    "        \n",
    "        save_path = os.path.join(home, f'ABSA/outputs/mvp', f'f1_{data_type}_{llm}_{f1_type}.xlsx')\n",
    "        print(f'finished and save data ==>> {save_path}')\n",
    "        \n",
    "        fi_score.to_excel(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9ecf88d4-8d5a-4be8-9bdd-3ddc282dd4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asqp_rest15 : 75731 data add. Total number: 75731\n",
      "asqp_rest16 : 78631 data add. Total number: 154362\n",
      "acos_rest16 : 81131 data add. Total number: 235493\n",
      "acos_laptop16 : 42751 data add. Total number: 278244\n",
      "finished and save data ==>> /home/elicer/ABSA/outputs/mvp/asqp_rest15_train_zero_all.xlsx, Total number : 48598\n",
      "finished and save data ==>> /home/elicer/ABSA/outputs/mvp/asqp_rest16_train_zero_all.xlsx, Total number : 34458\n",
      "finished and save data ==>> /home/elicer/ABSA/outputs/mvp/acos_rest16_train_zero_all.xlsx, Total number : 43847\n",
      "finished and save data ==>> /home/elicer/ABSA/outputs/mvp/acos_laptop16_train_zero_all.xlsx, Total number : 151341\n",
      "asqp_rest15 : 12490 data add. Total number: 12490\n",
      "asqp_rest16 : 11230 data add. Total number: 23720\n",
      "acos_rest16 : 15388 data add. Total number: 39108\n",
      "acos_laptop16 : 12142 data add. Total number: 51250\n",
      "finished and save data ==>> /home/elicer/ABSA/outputs/mvp/asqp_rest15_dev_zero_all.xlsx, Total number : 12509\n",
      "finished and save data ==>> /home/elicer/ABSA/outputs/mvp/asqp_rest16_dev_zero_all.xlsx, Total number : 15958\n",
      "finished and save data ==>> /home/elicer/ABSA/outputs/mvp/acos_rest16_dev_zero_all.xlsx, Total number : 6916\n",
      "finished and save data ==>> /home/elicer/ABSA/outputs/mvp/acos_laptop16_dev_zero_all.xlsx, Total number : 15867\n"
     ]
    }
   ],
   "source": [
    "# aggregate 10 of data by task, dataset \n",
    "method = 'mvp'\n",
    "llm = 'zero'\n",
    "\n",
    "data_types = ['train', 'dev']\n",
    "task_datasets = [['asqp','rest15'],['asqp','rest16'],['acos','rest16'],['acos','laptop16']]\n",
    "data_counts =  [[834, 606, 723, 2934],[209, 277, 133, 326]]\n",
    "home = os.path.expanduser('~/')\n",
    "\n",
    "for data_type in data_types:\n",
    "    tot_df = pd.DataFrame()\n",
    "    for task_dataset in task_datasets:\n",
    "        task, dataset = task_dataset[0], task_dataset[1]\n",
    "    \n",
    "        if method == 'mvp':\n",
    "            output_dir = os.path.join(home, f'ABSA/outputs/{method}/{task}/{dataset}/top_5_post_data1.0')\n",
    "        elif method == 'dlo':\n",
    "            output_dir = os.path.join(home, f'ABSA/outputs/{method}/{task}/{dataset}/top_5_post_data1.0')\n",
    "        elif method == 'paraphrase':\n",
    "            output_dir = os.path.join(home, f'ABSA/outputs/{method}/{task}/{dataset}/post_data1.0')\n",
    "    \n",
    "        task_df = pd.DataFrame()\n",
    "        for i in range(0, 11):\n",
    "    \n",
    "            file_name = f'{task}_{dataset}_{data_type}_{llm}_{i}.xlsx'\n",
    "            file_path = os.path.join(output_dir, file_name)\n",
    "            df = pd.read_excel(file_path, index_col=False)\n",
    "            \n",
    "            df['model'] = f'{task}_{dataset}'\n",
    "            \n",
    "            df['dataset'] = ''\n",
    "            start_index = 1\n",
    "            \n",
    "            if data_type == 'train':\n",
    "                data_count = data_counts[0]\n",
    "            elif data_type == 'dev':\n",
    "                data_count = data_counts[1]\n",
    "            \n",
    "            for count, name in zip(data_count, task_datasets):\n",
    "                task1, dataset1 = name[0], name[1]\n",
    "                if task!=task1 or dataset!=dataset1:\n",
    "                    end_index = start_index + count - 1\n",
    "                    df.loc[df['sent_id'].between(start_index, end_index), 'dataset'] = f'{task1}_{dataset1}'\n",
    "                    sent_ids = df.loc[df['sent_id'].between(start_index, end_index), 'sent_id'].unique()\n",
    "                    data_id_map = {sid: i+1 for i, sid in enumerate(sorted(sent_ids))}\n",
    "                    for sid, data_id in data_id_map.items():\n",
    "                        df.loc[df['sent_id'] == sid, 'data_id'] = data_id\n",
    "                    start_index = end_index + 1  # 다음 dataset 처리를 위한 start_index 업데이트\n",
    "    \n",
    "            df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "            cols_to_front = ['model', 'dataset', 'data_id']\n",
    "            df = df[cols_to_front + [col for col in df.columns if col not in cols_to_front]]\n",
    "    \n",
    "            task_df = pd.concat([task_df, df], ignore_index=True)\n",
    "        tot_df = pd.concat([tot_df, task_df], ignore_index=True)\n",
    "        print(f'{task}_{dataset} : {len(task_df)} data add. Total number: {len(tot_df)}')\n",
    "    \n",
    "    for task_dataset in task_datasets:\n",
    "        task, dataset = task_dataset[0], task_dataset[1]\n",
    "    \n",
    "        save_path = os.path.join(home, f'ABSA/outputs/mvp', f'{task}_{dataset}_{data_type}_{llm}_all.xlsx')\n",
    "        save_df = tot_df[tot_df['dataset']==f'{task}_{dataset}']\n",
    "        save_df.to_excel(save_path)\n",
    "        print(f'finished and save data ==>> {save_path}, Total number : {len(save_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9e0aaf-e791-42f3-bbec-0dc4a11b118b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa9488b-13c6-4c75-8319-8ff07948f584",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atoss",
   "language": "python",
   "name": "atoss"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
